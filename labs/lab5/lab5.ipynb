{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1aj0hFYUGpy9RKBm8_3NlMWECQxb5o__G",
      "authorship_tag": "ABX9TyP9rXNO6OVV8vsKCpH9zHdj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sentence_transformers import InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/nlp_7th_sem/lab5\"\n",
        "\n",
        "CORPUS_FILE = f\"{BASE_PATH}/validation_corpus.csv\"\n",
        "QUERIES_FILE = f\"{BASE_PATH}/validation_queries.csv\"\n",
        "TRIPLET_DATASET_NAME = f\"{BASE_PATH}/train_triplet.csv\""
      ],
      "metadata": {
        "id": "oB9CpX4PeI4m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_validation_data(corpus_path, queries_path):\n",
        "    print(\"Loading Validation Corpus and Queries...\")\n",
        "\n",
        "    try:\n",
        "        df_corpus = pd.read_csv(corpus_path, sep=\"|\")\n",
        "    except Exception:\n",
        "        df_corpus = pd.read_csv(corpus_path)\n",
        "\n",
        "    df_corpus['display_name'] = df_corpus['display name'].fillna('')\n",
        "    df_corpus['description'] = df_corpus['description'].fillna('')\n",
        "\n",
        "    df_corpus['combined_text'] = (\n",
        "        df_corpus['display_name'] + \" \" + df_corpus['description']\n",
        "    )\n",
        "\n",
        "    corpus_documents = df_corpus['combined_text'].tolist()\n",
        "\n",
        "    print(f\"Corpus loaded: {len(corpus_documents)} documents (Combined 'display name' and 'description').\")\n",
        "\n",
        "    df_queries = pd.read_csv(\n",
        "        queries_path,\n",
        "        sep=\"|\",\n",
        "        converters={'expected_results': pd.eval}\n",
        "    )\n",
        "    print(f\"Queries loaded: {len(df_queries)} queries.\")\n",
        "\n",
        "    return corpus_documents, df_queries\n",
        "\n",
        "\n",
        "def load_and_prepare_triplet_data(dataset_path):\n",
        "    print(f\"\\nLoading and preparing Triplet Training Data from: {dataset_path}...\")\n",
        "\n",
        "    try:\n",
        "        df_train = pd.read_csv(dataset_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Training file '{dataset_path}' not found. Cannot proceed with training.\")\n",
        "        return None, 0\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading training file '{dataset_path}': {e}\")\n",
        "        return None, 0\n",
        "\n",
        "    train_examples = []\n",
        "    for _, row in df_train.iterrows():\n",
        "        try:\n",
        "            train_examples.append(\n",
        "                InputExample(\n",
        "                    texts=[row['anchor'], row['positive'], row['negative']]\n",
        "                )\n",
        "            )\n",
        "        except KeyError as e:\n",
        "            print(f\"Error: Missing required column {e}. Please ensure columns are 'anchor', 'positive', 'negative'.\")\n",
        "            return None, 0\n",
        "\n",
        "    print(f\"Training examples prepared: {len(train_examples)} triplets.\")\n",
        "\n",
        "    # Create a PyTorch DataLoader\n",
        "    batch_size = 32\n",
        "\n",
        "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
        "    print(f\"DataLoader created with batch size: {batch_size}\")\n",
        "\n",
        "    return train_dataloader, len(train_examples)\n"
      ],
      "metadata": {
        "id": "sb48IYPpfBAm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_documents, df_validation_queries = load_validation_data(CORPUS_FILE, QUERIES_FILE)\n",
        "\n",
        "train_dataloader, num_train_examples = load_and_prepare_triplet_data(TRIPLET_DATASET_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SOxEe4afF39",
        "outputId": "ea5699c7-300d-4a00-f2c0-a6231e73d566"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Validation Corpus and Queries...\n",
            "Corpus loaded: 5000 documents (Combined 'display name' and 'description').\n",
            "Queries loaded: 55 queries.\n",
            "\n",
            "Loading and preparing Triplet Training Data from: /content/drive/MyDrive/nlp_7th_sem/lab5/train_triplet.csv...\n",
            "Training examples prepared: 10000 triplets.\n",
            "DataLoader created with batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
        "\n",
        "pooling_model = models.Pooling(\n",
        "    word_embedding_model.get_word_embedding_dimension(),\n",
        "    pooling_mode='mean'\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "metadata": {
        "id": "qfssVkAfjo-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "num_epochs = 4\n",
        "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\n",
        "train_loss = losses.TripletLoss(model=model)\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=num_epochs,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=f'{BASE_PATH}/sentence_transformer_model',\n",
        "    show_progress_bar=True\n",
        ")\n",
        "print(\"Training complete. Model saved.\")"
      ],
      "metadata": {
        "id": "5dutneMBj0ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "print(\"\\nGenerating corpus embeddings...\")\n",
        "corpus_embeddings = model.encode(corpus_documents, show_progress_bar=True, convert_to_tensor=True)\n",
        "\n",
        "query_texts = df_validation_queries['queries'].tolist()\n",
        "print(\"Generating query embeddings...\")\n",
        "query_embeddings = model.encode(query_texts, show_progress_bar=True, convert_to_tensor=True)\n",
        "\n",
        "# 3. Calculate Similarity (e.g., Cosine Similarity)\n",
        "print(\"Calculating cosine similarities...\")\n",
        "# Transpose query_embeddings for easy matrix multiplication if using PyTorch/NumPy\n",
        "cos_scores = cosine_similarity(query_embeddings.cpu().numpy(), corpus_embeddings.cpu().numpy())\n",
        "\n",
        "# 4. Get the Top K results for each query\n",
        "TOP_K = 10\n",
        "all_results = []\n",
        "for i in tqdm(range(len(query_texts)), desc=\"Retrieving top results\"):\n",
        "    # Get the indices of the top K highest scores\n",
        "    top_indices = np.argsort(cos_scores[i])[-TOP_K:][::-1]\n",
        "    all_results.append(top_indices.tolist())\n",
        "\n",
        "# Add results to the DataFrame\n",
        "df_validation_queries['retrieved_results'] = all_results"
      ],
      "metadata": {
        "id": "6mR9Y00hkUYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for metrics\n",
        "def calculate_metrics(df, k=5):\n",
        "    precision_list, recall_list, f_score_list, avg_precision_list = [], [], [], []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        expected = set(row['expected_results'])\n",
        "        retrieved = row['retrieved_results'][:k]\n",
        "\n",
        "        relevant_retrieved = len(expected.intersection(retrieved))\n",
        "\n",
        "        # Precision@k: (Relevant Retrieved) / k\n",
        "        P_k = relevant_retrieved / k\n",
        "\n",
        "        # Recall@k: (Relevant Retrieved) / (Total Relevant)\n",
        "        R_k = relevant_retrieved / len(expected) if len(expected) > 0 else 0\n",
        "\n",
        "        F_k = (2 * P_k * R_k) / (P_k + R_k) if (P_k + R_k) > 0 else 0\n",
        "\n",
        "        precision_list.append(P_k)\n",
        "        recall_list.append(R_k)\n",
        "        f_score_list.append(F_k)\n",
        "\n",
        "        # Average Precision (AP) for MAP calculation (using all retrieved results)\n",
        "        ap = 0\n",
        "        hits = 0\n",
        "        for rank, doc_index in enumerate(row['retrieved_results']):\n",
        "            if doc_index in expected:\n",
        "                hits += 1\n",
        "                ap += hits / (rank + 1)\n",
        "\n",
        "        AP = ap / len(expected) if len(expected) > 0 else 0\n",
        "        avg_precision_list.append(AP)\n",
        "\n",
        "\n",
        "    return {\n",
        "        f'Precision@{k}': np.mean(precision_list),\n",
        "        f'Recall@{k}': np.mean(recall_list),\n",
        "        f'F-score@{k}': np.mean(f_score_list),\n",
        "        'MAP': np.mean(avg_precision_list)\n",
        "    }\n",
        "\n",
        "metrics = calculate_metrics(df_validation_queries, k=5)\n",
        "print(\"\\n--- Training Results (Post-Training) ---\")\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "UrXoipF9kmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Choose a random query to visualize\n",
        "query_index_to_plot = random.randint(0, len(query_texts) - 1)\n",
        "query_text_to_plot = query_texts[query_index_to_plot]\n",
        "top_10_indices = df_validation_queries.loc[query_index_to_plot, 'retrieved_results'][:10]\n",
        "expected_indices = set(df_validation_queries.loc[query_index_to_plot, 'expected_results'])\n",
        "\n",
        "# 1. Convert all embeddings to NumPy for UMAP\n",
        "all_embeddings_np = corpus_embeddings.cpu().numpy()\n",
        "query_embedding_np = query_embeddings.cpu().numpy()[query_index_to_plot].reshape(1, -1)\n",
        "combined_embeddings = np.vstack([all_embeddings_np, query_embedding_np])\n",
        "\n",
        "# 2. Apply UMAP to reduce dimensions to 2D\n",
        "print(f\"\\nApplying UMAP for visualization on query: '{query_text_to_plot}'\")\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "projected_embeddings = reducer.fit_transform(combined_embeddings)\n",
        "\n",
        "# Separate projected corpus and query points\n",
        "corpus_projected = projected_embeddings[:-1]\n",
        "query_projected = projected_embeddings[-1]\n",
        "\n",
        "# 3. Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "plt.scatter(corpus_projected[:, 0], corpus_projected[:, 1], c='gray', alpha=0.5, label='All Documents')\n",
        "\n",
        "top_10_projected = corpus_projected[top_10_indices]\n",
        "plt.scatter(top_10_projected[:, 0], top_10_projected[:, 1], c='blue', s=100, label=f'Top 10 Retrieved Results')\n",
        "\n",
        "plt.scatter(query_projected[0], query_projected[1], c='red', marker='X', s=200, label='Search Query (X)')\n",
        "\n",
        "expected_projected = corpus_projected[list(expected_indices.intersection(set(range(len(corpus_projected)))))]\n",
        "plt.scatter(expected_projected[:, 0], expected_projected[:, 1], c='green', marker='o', s=150, alpha=0.8, label='Expected Results')\n",
        "\n",
        "plt.title(f'UMAP Projection of Embeddings for Query: \"{query_text_to_plot}\"')\n",
        "plt.xlabel('First Component')\n",
        "plt.ylabel('Second Component')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7cqNEJb5kyJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}